{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/EAGLE/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.46it/s]\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /models/Meta-Llama-3-8B-Instruct and are newly initialized: ['model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EaModel(\n",
       "  (base_model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(128256, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "  )\n",
       "  (ea_layer): Model(\n",
       "    (embed_tokens): Embedding(128256, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "    (act): SiLU()\n",
       "    (logsoftmax): LogSoftmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from eagle.model.ea_model import EaModel\n",
    "\n",
    "\n",
    "model = EaModel.from_pretrained(\n",
    "    base_model_path='/models/Meta-Llama-3-8B-Instruct',\n",
    "    ea_model_path='yuhuili/EAGLE-LLaMA3-Instruct-8B',\n",
    "    total_token=60,\n",
    "    depth=5,\n",
    "    top_k=10,\n",
    "    #torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"cpu\"\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "tokenizer = model.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nGive a name of a color, answer in one word.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "\t{\n",
    "\t\t'role': 'user',\n",
    "\t\t'content': 'Give a name of a color, answer in one word.',\n",
    "\t}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[128000,\n",
       "  128006,\n",
       "  882,\n",
       "  128007,\n",
       "  271,\n",
       "  36227,\n",
       "  264,\n",
       "  836,\n",
       "  315,\n",
       "  264,\n",
       "  1933,\n",
       "  11,\n",
       "  4320,\n",
       "  304,\n",
       "  832,\n",
       "  3492,\n",
       "  13,\n",
       "  128009,\n",
       "  128006,\n",
       "  78191,\n",
       "  128007,\n",
       "  271]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer([prompt], add_special_tokens=False, ).input_ids\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eagle.model.kv_cache import initialize_past_key_values\n",
    "from eagle.model.utils import reset_tree_mode\n",
    "\n",
    "\n",
    "(\n",
    "    past_key_values,\n",
    "    past_key_values_data,\n",
    "    current_length_data,\n",
    ") = initialize_past_key_values(model.base_model)\n",
    "model.past_key_values = past_key_values\n",
    "model.past_key_values_data = past_key_values_data\n",
    "model.current_length_data = current_length_data\n",
    "\n",
    "reset_tree_mode(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPast(last_hidden_state=tensor([[[ 4.1851, -0.2059, -1.8382,  ..., -2.8908,  1.3605,  0.3110],\n",
       "         [-0.0892, -0.0484,  0.0638,  ...,  0.7794, -0.8125,  0.8029],\n",
       "         [ 2.6079,  0.7637, -3.9777,  ..., -0.0527, -0.3737,  1.9626],\n",
       "         ...,\n",
       "         [-2.6085,  4.6601, -4.3184,  ...,  0.4044, -0.4416,  0.1648],\n",
       "         [-2.3383,  1.4609, -2.3124,  ...,  2.0149,  0.0813, -0.9260],\n",
       "         [-2.5767,  3.5765,  6.0180,  ..., -0.2055,  2.0939, -0.1105]]]), past_key_values=(None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_tensor = torch.as_tensor(input_ids)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs, orig, hidden_states = model(\n",
    "        input_ids_tensor, past_key_values=past_key_values, output_orig=True\n",
    "    )\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 22, 4096]), torch.Size([1, 22, 4096]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape, hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22, 128256])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.1700e-08, 4.4491e-04, 2.8210e-07,  ..., 3.6314e-11, 3.6319e-11,\n",
       "        3.6317e-11])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_p = orig[0, -1].softmax(dim=-1)\n",
    "sample_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.3908, 0.1933, 0.1912, 0.0796, 0.0599, 0.0225, 0.0105, 0.0099, 0.0090,\n",
       "        0.0053]),\n",
       "indices=tensor([10544, 75613,  1451,  6161, 38062,    34,    53, 43069,    45, 48799]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk = torch.topk(sample_p, 10)\n",
    "topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Blue', 'Purple', 'Ind', 'Red', 'Tur', 'C', 'V', 'Orange', 'N', 'Yellow']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(topk.indices.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 21, 4096])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states = hidden_states[:, :-1]\n",
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 21, 4096])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_hidden, past_key_values = model.ea_layer(hidden_states, input_ids=input_ids_tensor[:, 1:], use_cache=True)\n",
    "out_hidden.shape, out_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128256])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden = out_hidden[:, -1]\n",
    "last_headout = model.base_model.lm_head(last_hidden)\n",
    "last_headout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128256]),\n",
       " tensor([[1.7842e-07, 1.0383e-02, 3.1208e-06,  ..., 3.0543e-07, 3.0549e-07,\n",
       "          3.0547e-07]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ea_p = last_headout.softmax(dim=-1)\n",
    "ea_p.shape, ea_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1553, 0.0490, 0.0010, 0.1466, 0.0018, 0.0069, 0.0102, 0.0328, 0.0003,\n",
       "        0.0461], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ea_top_p = ea_p[0, topk.indices]\n",
    "ea_top_p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
